 This video will be covering DIT or Diffusion Models with Transformers. For all our past diffusion videos, we have been working with a UNET architecture, where we had a series of encoder blocks, mid block and a series of up blocks, with skip connections between encoder and decoder blocks of same spatial resolution. The authors of DIT replaced this UNET architecture with a transformer, and similar to latent diffusion models, work on latent images rather than image in pixel space. For this, the authors base their architecture on Vision Transformer or VIT. The way we'll go about with this video is first to do a quick review of VIT. If you aren't familiar with VIT, then I've already covered it in detail in previous videos, so I would suggest to watch those first. Here, we'll only refresh some aspects of it. Then we'll refresh.